{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597321890746",
   "display_name": "Python 3.8.5 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomSearch(X, Y, function, cost_func, n_samples, _range):\n",
    "    best_weights = None ## no best weight found yet\n",
    "    best_bias = None ## no best bias found yet\n",
    "    lowest_cost = float('inf') ## initialize it very high (how high can it be?)\n",
    "    for i in range(n_samples): ## try this many different parameterisations\n",
    "        w = np.random.randn(-_range, _range) ## randomly sample a weight within the limits of the search\n",
    "        b = np.random.randn(-_range, _range) ## randomly sample a bias within the limits of the search\n",
    "        # print(w, b)\n",
    "        y_hat = function(X, w, b) ## make prediction\n",
    "        cost = cost_func(y_hat, Y) ## calculate loss\n",
    "        if cost < lowest_cost: ## if this is the best parameterisation so far\n",
    "            lowest_cost = cost ## update the lowest running cost to the cost for this parameterisation\n",
    "            best_weights = w ## get best weights so far from the model\n",
    "            best_bias = b ## get best bias so far from the model\n",
    "    print('Lowest cost of', lowest_cost, 'achieved with weight of', best_weights, 'and bias of', best_bias)\n",
    "    return best_weights, best_bias "
   ]
  }
 ]
}